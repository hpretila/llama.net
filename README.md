# LLaMA.NET - Rewritten
[![License: MIT](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)

.NET library to run [LLaMA](https://arxiv.org/abs/2302.13971) using [ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp). 

## Build üß∞
To build the library, you need to have [CMake](https://cmake.org/) and Python installed. Then, run the following commands at the root of the repository.

```bash

# Build and prepare the C++ library
python scripts/build_llama_cpp.py
```

Then, build the .NET library using `dotnet`:

```bash
# Build the .NET library
dotnet build LLaMA.NET/LLaMA.NET.csproj
```

The built library should be located at `LLaMA.NET/bin/Debug/netXXXX/LLaMA.NET.dll`.

Currently only Linux is supported. Work is being done to dynamically load the C++ library on other platforms.

## Usage üìñ

### Inference
To run inference, you need to load a model and create a runner. The runner can then be used to run inference on a prompt.
```csharp
public static class Program
{
    static Stopwatch sw = Stopwatch.StartNew();
    static int tokens=0;
    static double tokensPerSecond=0;
    private static void Main()
    {
        var model = LLaMAModel.FromPath("/models/ggml-alpaca-7b-native-q4.bin");
        var runner = model.CreateRunner(6);

        tokens = 0;
        runner.Instruction($"Write a C# script that displays the current date", "");
        sw = Stopwatch.StartNew();
        foreach(var token in runner.InferenceStream(150)) 
        {
            tokens++;
            Console.Write(token);
        }
        Console.WriteLine();
        tokensPerSecond = tokens / sw.Elapsed.TotalSeconds;
        Console.WriteLine($"Time: {sw.ElapsedMilliseconds:0.00}ms (tokens per second: {tokensPerSecond:0.00})");

        runner.Clear();

        tokens = 0;
        runner.Instruction($"Extract favorite food, Age, Name, City", "Hello my name is Trbl and I am 30 years old. I live in vienna austria. I like pizza. I hate fish. Spaghetti is good too");
        sw = Stopwatch.StartNew();
        foreach(var token in runner.InferenceStream(150)) 
        {
            tokens++;
            Console.Write(token);
        }
        Console.WriteLine();
        tokensPerSecond = tokens / sw.Elapsed.TotalSeconds;
        Console.WriteLine($"Time: {sw.ElapsedMilliseconds:0.00}ms (tokens per second: {tokensPerSecond:0.00})");

        runner.Clear();
       
        tokens = 0;
        runner.Continuation($"A long time ago, in a galaxy far far away... ");
        sw = Stopwatch.StartNew();
        foreach(var token in runner.InferenceStream(150)) 
        {
            tokens++;
            Console.Write(token);
        }
        Console.WriteLine();
        tokensPerSecond = tokens / sw.Elapsed.TotalSeconds;
        Console.WriteLine($"Time: {sw.ElapsedMilliseconds:0.00}ms (tokens per second: {tokensPerSecond:0.00})");

        runner.Clear();

        tokens = 0;

        var story = @"""We buried my brother with his dreams. On colored scraps of paper my young son, Teddy, and I scrawled all the fantasies Abe never achieved for lack of trying: hero, quarterback, singer, actor and more and crammed them in the satin folds of his coffin along with his favorite bottle of Jack and a pack of Camels. Teddy, a budding artist, sketched Abe throwing a football.
            ‚ÄúCan you imagine Uncle Abe throwing long on a cloud?‚Äù Teddy asked as he gingerly dropped in the drawing.
            ‚ÄúMight piss off the angels if he gets too rowdy,‚Äù I shrugged. ‚ÄúSame goes for showing off his bravery or acting like he‚Äôs better than all the other souls.‚Äù
            ‚ÄúEveryone sings in heaven. He can sing, huh?‚Äù Teddy pressed.
            ‚ÄúNot off-key. God has sensitive ears.‚Äù
            ‚ÄúSo, Uncle Abe doesn‚Äôt get to live his dreams after all? That sucks,‚Äù Teddy gathered his crayons and paper, sat on the floor of the funeral parlor and begin drawing in earnest.
            ‚ÄúWhat the hell are you doing, Teddy?‚Äù
            Teddy put a finishing flourish on a portrait of himself painting.
            ‚ÄúGoing for my dreams while I can just in case I run out of time and end up in heaven.‚Äù""";

        runner.Instruction($"What is this story about?", story);
        sw = Stopwatch.StartNew();
        foreach(var token in runner.InferenceStream(150)) 
        {
            tokens++;
            Console.Write(token);
        }
        Console.WriteLine();
        tokensPerSecond = tokens / sw.Elapsed.TotalSeconds;
        Console.WriteLine($"Time: {sw.ElapsedMilliseconds:0.00}ms (tokens per second: {tokensPerSecond:0.00})");

```

## License üìú
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments üôè
- [hpretila/llama.net](https://github.com/hpretila/llama.net) The original author
- [ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp) for the LLaMA implementation in C++.
- [sandrohanea/whisper.net](https://github.com/sandrohanea/whisper.net) as the reference on loading ggml models and libraries into .NET.